{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fff3b4f-ebd6-4f37-8352-15b60f414d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: datasets 2.21.0 has requirement requests>=2.32.2, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas transformers peft accelerate sentencepiece datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a0b58-f914-406a-8ba0-96ac025f9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "# import os\n",
    "# model_dir = snapshot_download('qwen/Qwen2-7B-Instruct', cache_dir='/root/autodl-tmp', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079aef7e-1924-4d84-adb0-3597a75032ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
    "from peft import PeftModel,LoraConfig, TaskType, get_peft_model\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53521478-8ab9-474e-9f37-4ef4d61b7104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('./train.csv', encoding='gb2312', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a7fa9f-57ea-4dc9-b178-5ee084c746c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./test.csv', encoding='gb2312', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a04e7b-15e9-44ea-8047-f3a7f1570150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3500 entries, 0 to 3499\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   数据编号    3500 non-null   object\n",
      " 1   创作要求    2000 non-null   object\n",
      " 2   评判维度    3500 non-null   object\n",
      " 3   待评判内容   3475 non-null   object\n",
      " 4   标注分值    3500 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 136.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015c06da-f429-43d9-8ce1-e615a4aefc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3049 entries, 0 to 3048\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   数据编号    3049 non-null   object\n",
      " 1   创作要求    2049 non-null   object\n",
      " 2   评判维度    3049 non-null   object\n",
      " 3   待评判内容   3025 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 95.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb77c97e-ce6b-4b57-bfa7-993560379784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>数据编号</th>\n",
       "      <th>创作要求</th>\n",
       "      <th>评判维度</th>\n",
       "      <th>待评判内容</th>\n",
       "      <th>标注分值</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TR0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>流畅性</td>\n",
       "      <td>美国塔科马市（47°17'N，122°28'W，是北太平洋东岸的港口城市，位于，人口约21....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TR0002</td>\n",
       "      <td>请以“针对中国人民银行及其各级分支机构，以及各类银行机构（包括政策性银行、国有商业银行、中国...</td>\n",
       "      <td>规范性</td>\n",
       "      <td>中国人民银行上海总部，各省、自治区、直辖市及计划单列市分行；各政策性银行、国有商业银行，中国...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     数据编号                                               创作要求 评判维度  \\\n",
       "0  TR0001                                                NaN  流畅性   \n",
       "1  TR0002  请以“针对中国人民银行及其各级分支机构，以及各类银行机构（包括政策性银行、国有商业银行、中国...  规范性   \n",
       "\n",
       "                                               待评判内容  标注分值  \n",
       "0  美国塔科马市（47°17'N，122°28'W，是北太平洋东岸的港口城市，位于，人口约21....     3  \n",
       "1  中国人民银行上海总部，各省、自治区、直辖市及计划单列市分行；各政策性银行、国有商业银行，中国...     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc8823f-f292-4b19-8478-8c34a01addf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>数据编号</th>\n",
       "      <th>创作要求</th>\n",
       "      <th>评判维度</th>\n",
       "      <th>待评判内容</th>\n",
       "      <th>标注分值</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>CH0499</td>\n",
       "      <td>边际贡献率和变动成本率____。 选择正确选项作答  A、反方向变化 B、同方向变化 C、同...</td>\n",
       "      <td>选择题</td>\n",
       "      <td>正确选项为A 模型回复为根据题干中的信息，我们可以推断出 反比例变化</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>CH0500</td>\n",
       "      <td>$HO_2$的沸点比$H_2S$的高,可以从____角度来进行解释。 选择正确选项作答  A...</td>\n",
       "      <td>选择题</td>\n",
       "      <td>正确选项为C 模型回复为C:氢键</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        数据编号                                               创作要求 评判维度  \\\n",
       "3498  CH0499  边际贡献率和变动成本率____。 选择正确选项作答  A、反方向变化 B、同方向变化 C、同...  选择题   \n",
       "3499  CH0500  $HO_2$的沸点比$H_2S$的高,可以从____角度来进行解释。 选择正确选项作答  A...  选择题   \n",
       "\n",
       "                                   待评判内容  标注分值  \n",
       "3498  正确选项为A 模型回复为根据题干中的信息，我们可以推断出 反比例变化     0  \n",
       "3499                   正确选项为C 模型回复为C:氢键      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c2eeb5-ae32-4418-876b-7ad4c9846cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>数据编号</th>\n",
       "      <th>创作要求</th>\n",
       "      <th>评判维度</th>\n",
       "      <th>待评判内容</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TE0001</td>\n",
       "      <td>请以“描述一个初冬早晨的校园场景，展现了学生们在学业压力与日常生活小插曲中的生动画面，特别是...</td>\n",
       "      <td>规范性</td>\n",
       "      <td>那是一个潮湿的初冬的早上，学校那栋白色的教学楼像是融化在白雾里，只看见透明的或是绿色的玻璃窗...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TE0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>流畅性</td>\n",
       "      <td>2024年春节临近，山西博物院推出多款文创产品和多场展览推出，前来参观的游客不少吸引。\\n2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     数据编号                                               创作要求 评判维度  \\\n",
       "0  TE0001  请以“描述一个初冬早晨的校园场景，展现了学生们在学业压力与日常生活小插曲中的生动画面，特别是...  规范性   \n",
       "1  TE0002                                                NaN  流畅性   \n",
       "\n",
       "                                               待评判内容  \n",
       "0  那是一个潮湿的初冬的早上，学校那栋白色的教学楼像是融化在白雾里，只看见透明的或是绿色的玻璃窗...  \n",
       "1  2024年春节临近，山西博物院推出多款文创产品和多场展览推出，前来参观的游客不少吸引。\\n2...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2398ae2b-b389-4a93-adb5-379bc7763687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>数据编号</th>\n",
       "      <th>创作要求</th>\n",
       "      <th>评判维度</th>\n",
       "      <th>待评判内容</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>CHT1048</td>\n",
       "      <td>1948年底到1949年初，中共中央确定了一个重要的外交方针，将承认问题由新政权的单向“被承...</td>\n",
       "      <td>选择题</td>\n",
       "      <td>正确选项为D 模型回复为The correct answer is  “求同存异”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>CHT1049</td>\n",
       "      <td>新冠肺炎疫情防控期间，“天价口罩”凸显了相关企业违反的商业伦理类型是 选择正确选项作答  A...</td>\n",
       "      <td>选择题</td>\n",
       "      <td>正确选项为D 模型回复为D.操纵价格行为</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         数据编号                                               创作要求 评判维度  \\\n",
       "3047  CHT1048  1948年底到1949年初，中共中央确定了一个重要的外交方针，将承认问题由新政权的单向“被承...  选择题   \n",
       "3048  CHT1049  新冠肺炎疫情防控期间，“天价口罩”凸显了相关企业违反的商业伦理类型是 选择正确选项作答  A...  选择题   \n",
       "\n",
       "                                           待评判内容  \n",
       "3047  正确选项为D 模型回复为The correct answer is  “求同存异”   \n",
       "3048                        正确选项为D 模型回复为D.操纵价格行为  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060219b2-f244-49d5-80ec-bfb65e192c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(train_df)):\n",
    "    llm_item = train_df.loc[i]\n",
    "    if (llm_item['评判维度']==\"选择题\"):\n",
    "        tmp =  {\n",
    "        \"instruction\": \"如果模型回复与正确选项一致，则输出1；如果不一致，则输出0。\",\n",
    "        \"input\": f\"{llm_item['创作要求']}。{llm_item['待评判内容']}\",\n",
    "        \"output\": str(llm_item['标注分值']) \n",
    "        }\n",
    "        res.append(tmp)\n",
    "\n",
    "    elif (llm_item['评判维度']==\"流畅性\"):\n",
    "        tmp =   {\n",
    "        \"instruction\": \"\"\"下面是一个模型完成的创作内容。按照流畅性评分标准给模型创作打分(只取1分、2分、3分、4分、5分其一)。流畅性评分标准如下：\n",
    "        1分: 非常不流畅，不具备可读性，语法错误明显，难以理解，大量拼写错误和错别字，影响阅读，表达不清晰，难以捉摸要表达的意思，每百字平均错误数超过2.5个。\n",
    "        2分: 具有可读性，但较不流畅，常见语法错误多，需花费一定时间理解，一些拼写错误和错别字，阅读中断，表达较为模糊，需用一些猜测才能明白含义，每百字平均错误数介于2至2.5个。\n",
    "        3分：基本流畅，存在少量语法错误，但影响较小，稍有拼写错误，但不影响阅读，主要意思表达清楚，但部分地方表述不够准确，每百字平均错误数介于1至2个。\n",
    "        4分：较流畅，语法错误稀少，易读性较高，几乎无拼写错误，阅读顺畅，表达清晰、准确，容易理解，每百字平均错误数介于0.5至1个。\n",
    "        5分：非常流畅，语法、拼写完美，阅读体验优秀，表达精炼、准确、得体；文句优美，行文连贯，思维严密，每百字平均错误数在0至0.5个之间。\"\"\",\n",
    "        \"input\": f\" 模型创作:{llm_item['待评判内容']}\",\n",
    "        \"output\": str(llm_item['标注分值']) \n",
    "        }\n",
    "        res.append(tmp)\n",
    "\n",
    "    else:\n",
    "        tmp ={\n",
    "        \"instruction\": \"\"\"下面是一个模型，根据创作要求，完成的创作内容。按照规范性评分标准给模型创作打分(只取1分、2分、3分、4分、5分其一)。规范性评分标准如下：\n",
    "        1分: 创作内容离题，与提示语句要求不符，格式非常不规范，杂乱无章，句子结构混乱，缺乏逻辑，每千字平均错误数超过5个。\n",
    "        2分: 创作内容与提示语句要求有一定契合但覆盖不全，格式较不规范：缺乏清晰的结构，但基本逻辑仍能找到，每千字平均错误数在4至5个之间。\n",
    "        3分：创作内容与提示语句要求基本契合但覆盖不全，格式一般规范，结构基本顺畅，逻辑较清晰，每千字平均错误数在2至4个之间。\n",
    "        4分：创作内容与提示语句要求基本契合且基本覆盖，格式较规范，结构清晰，逻辑条理分明，每千字平均错误数在1至2个之间。\n",
    "        5分：创作内容与提示语句要求完美契合，格式非常规范：结构严谨，逻辑清晰，层次分明，每千字平均错误数在0至1个之间。\"\"\",\n",
    "        \"input\": f\"创作要求:{llm_item['创作要求']}。模型创作:{llm_item['待评判内容']}\",\n",
    "        \"output\": str(llm_item['标注分值']) \n",
    "        }\n",
    "        res.append(tmp)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ba5565-ec7d-453f-8671-3378d764d7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "394bdd77-f234-4b8b-9a44-e7b8ff80ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_builtin_type(obj):\n",
    "    if isinstance(obj, np.int64):  # 如果对象是 int64 类型\n",
    "        return int(obj)  # 转换为内置的 int 类型\n",
    "    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "with open('./train_glm4.json', mode='w', encoding='utf-8') as f:\n",
    "    json.dump(res, f, ensure_ascii=False, indent=4, default=convert_to_builtin_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f591d422-e173-454e-8b01-1488cb20d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:56:36,984 - modelscope - INFO - PyTorch version 2.1.2 Found.\n",
      "2024-09-10 22:56:36,989 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-09-10 22:56:42,663 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-09-10 22:56:43,629 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 f6e031fdbad3477fb3f2c28e57d6c348 and a total number of 976 components indexed\n",
      "2024-09-10 22:56:44,458 - modelscope - WARNING - Authentication has expired, please re-login if you need to access private models or datasets.\n",
      "Downloading: 100%|██████████| 1.40k/1.40k [00:00<00:00, 12.7MB/s]\n",
      "Downloading: 100%|██████████| 36.0/36.0 [00:00<00:00, 353kB/s]\n",
      "Downloading: 100%|██████████| 2.21k/2.21k [00:00<00:00, 21.8MB/s]\n",
      "Downloading: 100%|██████████| 207/207 [00:00<00:00, 1.90MB/s]\n",
      "Downloading: 100%|██████████| 6.34k/6.34k [00:00<00:00, 43.6MB/s]\n",
      "Downloading: 100%|█████████▉| 1.81G/1.81G [02:03<00:00, 15.8MB/s]\n",
      "Downloading: 100%|█████████▉| 1.69G/1.69G [01:36<00:00, 18.9MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [01:47<00:00, 18.4MB/s]\n",
      "Downloading: 100%|█████████▉| 1.80G/1.80G [01:25<00:00, 22.4MB/s]\n",
      "Downloading: 100%|█████████▉| 1.69G/1.69G [01:05<00:00, 27.7MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [01:02<00:00, 31.4MB/s]\n",
      "Downloading: 100%|█████████▉| 1.80G/1.80G [00:51<00:00, 37.2MB/s]\n",
      "Downloading: 100%|█████████▉| 1.69G/1.69G [00:42<00:00, 43.1MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [00:48<00:00, 40.7MB/s]\n",
      "Downloading: 100%|█████████▉| 1.54G/1.54G [01:14<00:00, 22.2MB/s]\n",
      "Downloading: 100%|██████████| 28.4k/28.4k [00:00<00:00, 5.24MB/s]\n",
      "Downloading: 100%|██████████| 46.2k/46.2k [00:00<00:00, 3.76MB/s]\n",
      "Downloading: 100%|██████████| 8.71k/8.71k [00:00<00:00, 32.7MB/s]\n",
      "Downloading: 100%|██████████| 9.29k/9.29k [00:00<00:00, 71.1MB/s]\n",
      "Downloading: 100%|██████████| 20.5k/20.5k [00:00<00:00, 4.75MB/s]\n",
      "Downloading: 100%|██████████| 2.50M/2.50M [00:00<00:00, 41.3MB/s]\n",
      "Downloading: 100%|██████████| 6.01k/6.01k [00:00<00:00, 41.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "\n",
    "model_dir = snapshot_download('ZhipuAI/glm-4-9b-chat', cache_dir='/group_share/glm-4-9b-chat', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67072ba8-1cb1-42b1-b333-e04f59568106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 2048\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer((f\"[gMASK]<sop><|system|>\\n您将担任评审的专家角色。<|user|>\\n\"\n",
    "                            f\"{example['instruction']+example['input']}<|assistant|>\\n\"\n",
    "                            ).strip(), \n",
    "                            add_special_tokens=False)\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4e25be-d129-4cd1-8857-54c60c7c8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./train_glm4.json')\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f624d1b6-c660-4c23-9715-ff01d15286a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['下面是一个模型完成的创作内容。按照流畅性评分标准给模型创作打分(只取1分、2分、3分、4分、5分其一)。流畅性评分标准如下：\\n        1分: 非常不流畅，不具备可读性，语法错误明显，难以理解，大量拼写错误和错别字，影响阅读，表达不清晰，难以捉摸要表达的意思，每百字平均错误数超过2.5个。\\n        2分: 具有可读性，但较不流畅，常见语法错误多，需花费一定时间理解，一些拼写错误和错别字，阅读中断，表达较为模糊，需用一些猜测才能明白含义，每百字平均错误数介于2至2.5个。\\n        3分：基本流畅，存在少量语法错误，但影响较小，稍有拼写错误，但不影响阅读，主要意思表达清楚，但部分地方表述不够准确，每百字平均错误数介于1至2个。\\n        4分：较流畅，语法错误稀少，易读性较高，几乎无拼写错误，阅读顺畅，表达清晰、准确，容易理解，每百字平均错误数介于0.5至1个。\\n        5分：非常流畅，语法、拼写完美，阅读体验优秀，表达精炼、准确、得体；文句优美，行文连贯，思维严密，每百字平均错误数在0至0.5个之间。',\n",
       "  '下面是一个模型，根据创作要求，完成的创作内容。按照规范性评分标准给模型创作打分(只取1分、2分、3分、4分、5分其一)。规范性评分标准如下：\\n        1分: 创作内容离题，与提示语句要求不符，格式非常不规范，杂乱无章，句子结构混乱，缺乏逻辑，每千字平均错误数超过5个。\\n        2分: 创作内容与提示语句要求有一定契合但覆盖不全，格式较不规范：缺乏清晰的结构，但基本逻辑仍能找到，每千字平均错误数在4至5个之间。\\n        3分：创作内容与提示语句要求基本契合但覆盖不全，格式一般规范，结构基本顺畅，逻辑较清晰，每千字平均错误数在2至4个之间。\\n        4分：创作内容与提示语句要求基本契合且基本覆盖，格式较规范，结构清晰，逻辑条理分明，每千字平均错误数在1至2个之间。\\n        5分：创作内容与提示语句要求完美契合，格式非常规范：结构严谨，逻辑清晰，层次分明，每千字平均错误数在0至1个之间。'],\n",
       " 'input': [\" 模型创作:美国塔科马市（47°17'N，122°28'W，是北太平洋东岸的港口城市，位于，人口约21.9万（2020年）。该市一位名叫约克的年轻人，非常喜爱中国文化的，工作之余经常前往图书馆读书，或漫步公园游憩。1873年，横贯美洲大陆的北太平洋铁路建成。这条铁路成了分处铁路两端的约克曾祖父母缔结姻缘的纽带。他们初识时，塔科马位于铁路西端，千余人仅居住于此，到1889年人口达3.6万。图12示意塔科马市内部空间结构。\",\n",
       "  '创作要求:请以“针对中国人民银行及其各级分支机构，以及各类银行机构（包括政策性银行、国有商业银行、中国邮政储蓄银行和股份制商业银行）发布的一项通知或指示，但由于文章正文部分未给出，具体通知或指示的内容无法确定。因此，总结为：该文章是对上述金融机构发出的一个正式通知或指示，但具体内容和目的需要查看文章的详细内容才能得知。中国人民银行决定自2024年5月18日起下调个人住房公积金贷款利率，其中首套和二套房的贷款利率均有所降低。”为主题写一篇文章。模型创作:中国人民银行上海总部，各省、自治区、直辖市及计划单列市分行；各政策性银行、国有商业银行，中国邮政储蓄银行，各股份制商业银行：\\n中国人民银行位于上海的总部机构，以及遍布全国各省、自治区、直辖市以及计划单列市的分行；同时，各政策性银行、所有国有商业银行、中国邮政储蓄银行，以及各股份制商业银行均被告知：'],\n",
       " 'output': [3, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8613d9d-29a8-4668-860f-be9e3d53e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tiktoken\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/77/ba/e4149aa0724a5268ab10014a028668c6ef8834862f955ef0c6758240f6d1/tiktoken-0.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2024.7.24)\n",
      "Collecting requests>=2.26.0\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (1.25.8)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/3d/09/d82fe4a34c5f0585f9ea1df090e2a71eb9bb1e469723053e1ee9f57c16f3/charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Installing collected packages: charset-normalizer, requests, tiktoken\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.22.0\n",
      "    Not uninstalling requests at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "Successfully installed charset-normalizer-3.3.2 requests-2.32.3 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d641b35e-4379-4ce0-ba88-7a347f83de13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 10/10 [00:05<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat', device_map=\"auto\",torch_dtype=torch.bfloat16,trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('/group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat', use_fast=True,trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "277cce6b-732f-4cf2-a5c4-90fa7f3b3891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3500/3500 [00:05<00:00, 604.08 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3500\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c5c1e25-5c76-46e2-b822-7b231f04d64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK] <sop> <|system|> \n",
      "您将担任评审的专家角色。 <|user|> \n",
      "下面是一个模型完成的创作内容。按照流畅性评分标准给模型创作打分(只取1分、2分、3分、4分、5分其一)。流畅性评分标准如下：\n",
      "        1分: 非常不流畅，不具备可读性，语法错误明显，难以理解，大量拼写错误和错别字，影响阅读，表达不清晰，难以捉摸要表达的意思，每百字平均错误数超过2.5个。\n",
      "        2分: 具有可读性，但较不流畅，常见语法错误多，需花费一定时间理解，一些拼写错误和错别字，阅读中断，表达较为模糊，需用一些猜测才能明白含义，每百字平均错误数介于2至2.5个。\n",
      "        3分：基本流畅，存在少量语法错误，但影响较小，稍有拼写错误，但不影响阅读，主要意思表达清楚，但部分地方表述不够准确，每百字平均错误数介于1至2个。\n",
      "        4分：较流畅，语法错误稀少，易读性较高，几乎无拼写错误，阅读顺畅，表达清晰、准确，容易理解，每百字平均错误数介于0.5至1个。\n",
      "        5分：非常流畅，语法、拼写完美，阅读体验优秀，表达精炼、准确、得体；文句优美，行文连贯，思维严密，每百字平均错误数在0至0.5个之间。 模型创作:美国塔科马市（47°17'N，122°28'W，是北太平洋东岸的港口城市，位于，人口约21.9万（2020年）。该市一位名叫约克的年轻人，非常喜爱中国文化的，工作之余经常前往图书馆读书，或漫步公园游憩。1873年，横贯美洲大陆的北太平洋铁路建成。这条铁路成了分处铁路两端的约克曾祖父母缔结姻缘的纽带。他们初识时，塔科马位于铁路西端，千余人仅居住于此，到1889年人口达3.6万。图12示意塔科马市内部空间结构。 <|assistant|> 3 <|endoftext|>\n",
      "[151331, 151333, 151335, 198, 99526, 98493, 101638, 105724, 98314, 100153, 101053, 1773, 151336, 198, 101534, 99950, 100484, 109943, 100406, 99098, 1773, 99928, 108548, 98394, 104951, 99285, 98602, 100484, 100406, 98646, 98363, 7, 98552, 98721, 16, 98363, 5373, 17, 98363, 5373, 18, 98363, 5373, 19, 98363, 5373, 20, 98363, 114827, 100782, 108548, 98394, 104951, 99285, 101223, 28213, 286, 220, 16, 98363, 25, 18109, 251, 252, 98506, 98317, 108548, 3837, 116964, 98348, 98980, 98394, 3837, 108144, 101390, 99845, 3837, 101277, 100072, 3837, 101168, 127454, 101390, 98327, 99197, 98592, 98845, 3837, 98988, 100278, 3837, 100313, 98317, 102433, 3837, 101277, 104723, 101535, 98339, 100313, 106017, 3837, 98661, 98841, 98845, 100573, 101390, 98469, 100166, 17, 13, 20, 98328, 8994, 286, 220, 17, 98363, 25, 34199, 115, 98318, 98348, 98980, 98394, 3837, 98487, 98692, 98317, 108548, 3837, 101445, 108144, 101390, 98362, 3837, 98616, 107070, 99156, 98689, 100072, 3837, 99201, 127454, 101390, 98327, 99197, 98592, 98845, 3837, 100278, 110550, 3837, 100313, 102463, 104683, 3837, 98616, 98340, 99201, 109918, 99873, 101319, 104822, 3837, 98661, 98841, 98845, 100573, 101390, 98469, 122368, 17, 98634, 17, 13, 20, 98328, 8994, 286, 220, 18, 98363, 5122, 99277, 108548, 3837, 99295, 108251, 108144, 101390, 3837, 98487, 98988, 107301, 3837, 124240, 127454, 101390, 3837, 108717, 98988, 100278, 3837, 98924, 100893, 100313, 100962, 3837, 98487, 99130, 99483, 109225, 101945, 101367, 3837, 98661, 98841, 98845, 100573, 101390, 98469, 122368, 16, 98634, 17, 98328, 8994, 286, 220, 19, 98363, 5122, 98692, 108548, 3837, 108144, 101390, 101763, 98614, 3837, 98807, 98980, 98394, 103352, 3837, 100873, 98479, 127454, 101390, 3837, 100278, 114044, 3837, 100313, 102433, 5373, 101367, 3837, 99722, 100072, 3837, 98661, 98841, 98845, 100573, 101390, 98469, 122368, 15, 13, 20, 98634, 16, 98328, 8994, 286, 220, 20, 98363, 5122, 99088, 108548, 3837, 108144, 5373, 127454, 102615, 3837, 100278, 100290, 100125, 3837, 100313, 98672, 100714, 5373, 101367, 5373, 98398, 98386, 24892, 98400, 99432, 109322, 3837, 98351, 98400, 126841, 3837, 100845, 117000, 3837, 98661, 98841, 98845, 100573, 101390, 98469, 98319, 15, 98634, 15, 13, 20, 98328, 99420, 1773, 6567, 44143, 98604, 100406, 25, 99207, 100190, 98534, 98829, 98408, 9904, 101655, 11611, 99419, 6, 45, 3837, 115760, 11611, 99869, 6, 54, 3837, 98316, 98676, 108850, 98595, 100761, 98314, 107509, 99228, 3837, 100469, 3837, 100742, 98919, 99146, 13, 24, 98655, 9904, 115937, 15, 98334, 73696, 98711, 98408, 101209, 110061, 98919, 112443, 103594, 3837, 99088, 104885, 98549, 102921, 3837, 98538, 111326, 100356, 102571, 102429, 101547, 3837, 98543, 115366, 101314, 98747, 118721, 1773, 126182, 18, 98334, 3837, 100620, 100379, 110931, 102678, 98314, 98676, 108850, 101641, 103022, 1773, 105378, 101641, 100275, 98363, 98588, 101641, 98556, 109939, 98919, 98819, 99234, 100307, 100140, 108402, 98519, 101650, 100100, 98314, 116940, 1773, 98720, 99037, 98733, 98335, 3837, 100190, 98534, 98829, 100469, 101641, 98574, 99364, 3837, 99237, 106254, 99021, 102329, 108361, 3837, 98344, 117786, 24, 98334, 100742, 98690, 18, 13, 21, 98655, 1773, 98598, 98886, 120777, 100190, 98534, 98829, 98408, 100766, 99717, 99405, 1773, 151337, 18, 151329]\n",
      "[gMASK] <sop> <|system|>\n",
      "[151331, 151333, 151335]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_id[0]['input_ids']))\n",
    "print(tokenized_id[0]['input_ids'])\n",
    "print(tokenizer.decode([151331, 151333, 151335]))\n",
    "print(tokenizer.encode('[gMASK]<sop><|system|>', add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "468138ce-4068-4df7-b329-1edd0c2abcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 <|endoftext|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[0][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "964e872f-156d-48c3-bb01-63fabadfe474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (embedding): Embedding(\n",
       "      (word_embeddings): Embedding(151552, 4096)\n",
       "    )\n",
       "    (rotary_pos_emb): RotaryEmbedding()\n",
       "    (encoder): GLMTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-39): 40 x GLMBlock(\n",
       "          (input_layernorm): RMSNorm()\n",
       "          (self_attention): SelfAttention(\n",
       "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
       "            (core_attention): SdpaAttention(\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNorm()\n",
       "          (mlp): MLP(\n",
       "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
       "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): RMSNorm()\n",
       "    )\n",
       "    (output_layer): Linear(in_features=4096, out_features=151552, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5a4e78-4e7e-4f85-b22f-5954783e6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "782b1b1e-5450-4e75-a772-ddbc65feab54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5270cbe-a8dc-40c9-9d5f-c81bb0b97a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'query_key_value', 'dense', 'dense_4h_to_h', 'dense_h_to_4h'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],  # 现存问题只微调部分演示即可\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6faa1080-4d68-4a80-8435-79fe2d498d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'query_key_value', 'dense', 'dense_4h_to_h', 'dense_h_to_4h'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47449dc1-39ac-481e-a799-d34994075641",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bf2e5-aaed-4df7-85b3-cee461dddc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"/group_share/glm4_9B_chat_lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7b2573e-763c-43c1-aa34-1e46d6bd4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef0d16d4-45b6-45a2-b070-c7581d1dbe7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-10 23:23:18,009] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The default cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/xtuner0.1.17/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1090' max='1090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1090/1090 5:01:36, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.569200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.037600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/xtuner0.1.17/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/xtuner0.1.17/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1090, training_loss=0.34173109367353105, metrics={'train_runtime': 18113.4973, 'train_samples_per_second': 0.966, 'train_steps_per_second': 0.06, 'total_flos': 1.0802587377095148e+18, 'train_loss': 0.34173109367353105, 'epoch': 4.982857142857143})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() #不小心中断了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4db31fe-ac87-44e2-9425-457a5f7b2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    " \n",
    " \n",
    "def apply_lora(model_name_or_path, output_path, lora_path):\n",
    "    print(f\"Loading the base model from {model_name_or_path}\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    "    )\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    " \n",
    "    print(f\"Loading the LoRA adapter from {lora_path}\")\n",
    " \n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base,\n",
    "        lora_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    " \n",
    "    print(\"Applying the LoRA\")\n",
    "    model = lora_model.merge_and_unload()\n",
    " \n",
    "    print(f\"Saving the target model to {output_path}\")\n",
    "    model.save_pretrained(output_path)\n",
    "    base_tokenizer.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38d96fa4-ee68-4f61-8d3e-ef9456b58921",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"/group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat\"\n",
    "output_path = \"/group_share/glm4_9B_chat_instruct_review\"\n",
    "lora_path = \"/group_share/glm4_9B_chat_lora/checkpoint-1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cd576cc-e061-474b-ba53-eca4fecebc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the base model from /group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for /group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//group_share/glm-4-9b-chat/ZhipuAI/glm-4-9b-chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the LoRA adapter from /group_share/glm4_9B_chat_lora/checkpoint-1000\n",
      "Applying the LoRA\n",
      "Saving the target model to /group_share/glm4_9B_chat_instruct_review\n"
     ]
    }
   ],
   "source": [
    "apply_lora(model_name_or_path, output_path, lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8467ddc4-fd83-4d84-a399-baa7cb61370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting modelscope\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/58/bd/963488515859b641a2aa525ed20282e914dd4d3dc9471002f11b94e4a016/modelscope-1.18.0-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.8/dist-packages (from modelscope) (4.66.5)\n",
      "Collecting urllib3>=1.26\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/ca/1c/89ffc63a9605b583d5df2be791a27bc1a42b7c32bab68d3c8f2f73a98cd4/urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.8/dist-packages (from modelscope) (2.32.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.25->modelscope) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25->modelscope) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.25->modelscope) (2.8)\n",
      "Installing collected packages: urllib3, modelscope\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.8\n",
      "    Not uninstalling urllib3 at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'urllib3'. No files were found to uninstall.\n",
      "Successfully installed modelscope-1.18.0 urllib3-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "525e9e5c-de90-4698-95ac-3b38381362e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 9 not upgraded.\n",
      "Need to get 3316 kB of archives.\n",
      "After this operation, 11.1 MB of additional disk space will be used.\n",
      "Get:1 https://mirrors.aliyun.com/ubuntu focal/universe amd64 git-lfs amd64 2.9.2-1 [3316 kB]\n",
      "Fetched 3316 kB in 2s (1487 kB/s)  \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 27010 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_2.9.2-1_amd64.deb ...\n",
      "Unpacking git-lfs (2.9.2-1) ...\n",
      "Setting up git-lfs (2.9.2-1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b42f3-cd9c-4689-ac4f-73470fb2db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 10:04:39,215 - modelscope - INFO - [master 06569cf] 'upload model'\n",
      " 16 files changed, 2141 insertions(+), 52 deletions(-)\n",
      " delete mode 100644 README.md\n",
      " create mode 100644 added_tokens.json\n",
      " create mode 100644 config.json\n",
      " create mode 100644 configuration_chatglm.py\n",
      " create mode 100644 generation_config.json\n",
      " create mode 100644 model-00001-of-00004.safetensors\n",
      " create mode 100644 model-00002-of-00004.safetensors\n",
      " create mode 100644 model-00003-of-00004.safetensors\n",
      " create mode 100644 model-00004-of-00004.safetensors\n",
      " create mode 100644 model.safetensors.index.json\n",
      " create mode 100644 modeling_chatglm.py\n",
      " create mode 100644 special_tokens_map.json\n",
      " create mode 100644 tokenization_chatglm.py\n",
      " create mode 100644 tokenizer.model\n",
      " create mode 100644 tokenizer_config.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from modelscope.hub.api import HubApi\n",
    "\n",
    "YOUR_ACCESS_TOKEN = 'e65f0ae8-fd82-4626-a011-c1001e776ccd'\n",
    "\n",
    "api = HubApi()\n",
    "api.login(YOUR_ACCESS_TOKEN)\n",
    "api.push_model(\n",
    "    model_id=\"NumberJys/glm4_9B_chat_review_1000\", \n",
    "    model_dir=\"/group_share/glm4_9B_chat_instruct_review\" # 本地模型目录，要求目录中必须包含configuration.json\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43dd516e-0e33-4307-a679-8a7e876f38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /group_share/glm4_9B_chat_instruct_review contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//group_share/glm4_9B_chat_instruct_review.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for /group_share/glm4_9B_chat_instruct_review contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//group_share/glm4_9B_chat_instruct_review.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for /group_share/glm4_9B_chat_instruct_review contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//group_share/glm4_9B_chat_instruct_review.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:33<00:00, 23.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "model_path = '/group_share/glm4_9B_chat_instruct_review'\n",
    "# lora_path = '/group_share/glm4_9B_chat_lora/checkpoint-1000'\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,use_fast=True,trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\",torch_dtype=torch.bfloat16,trust_remote_code=True)\n",
    "\n",
    "# # 加载lora权重\n",
    "# model = PeftModel.from_pretrained(model, model_id=lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8af06f7d-d8be-4644-a328-46584cf3cbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "prompt = \"如果模型回复与正确选项一致，则输出1；如果不一致，则输出0。火灾自动报警系统可燃气体探测器，撤去可燃气体，探测器应在____内恢复到正常监视状态。 选择正确选项作答  A、60s B、80s C、100s D、120s。正确选项为A 模型回复为\\\"\\\"\\\"60s *\\\"\\\"\\\"\"\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"您将担任评审的专家角色\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2048, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    \n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b967de44-d406-42be-b384-05ac25626aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(text):\n",
    "    \n",
    "    prompt=f\"\"\"以下是一个大模型完成的选择题。如果大模型的回答与参考答案一致，输出1；如果不一致，输出0。\n",
    "            -->{text} \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"您将担任评审的专家角色\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 2048, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        \n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c68e021-917e-40d1-a655-a27494544e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(text):\n",
    "    \n",
    "    prompt=f\"\"\"下面是一个模型完成的创作内容。按照流畅性评分标准给模型创作打分(只取1分、2分、3分、4分、5分其一)。流畅性评分标准如下： \n",
    "            1分: 非常不流畅，不具备可读性，语法错误明显，难以理解，大量拼写错误和错别字，影响阅读，表达不清晰，难以捉摸要表达的意思，每百字平均错误数超过2.5个。 \n",
    "            2分: 具有可读性，但较不流畅，常见语法错误多，需花费一定时间理解，一些拼写错误和错别字，阅读中断，表达较为模糊，需用一些猜测才能明白含义，每百字平均错误数介于2至2.5个。 \n",
    "            3分：基本流畅，存在少量语法错误，但影响较小，稍有拼写错误，但不影响阅读，主要意思表达清楚，但部分地方表述不够准确，每百字平均错误数介于1至2个。 \n",
    "            4分：较流畅，语法错误稀少，易读性较高，几乎无拼写错误，阅读顺畅，表达清晰、准确，容易理解，每百字平均错误数介于0.5至1个。 \n",
    "            5分：非常流畅，语法、拼写完美，阅读体验优秀，表达精炼、准确、得体；文句优美，行文连贯，思维严密，每百字平均错误数在0至0.5个之间。\n",
    "            -->{text} \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"您将担任评审的专家角色\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 2048, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        \n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52542e13-7161-4f85-a4c6-5b75154c80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict3(text):\n",
    "    \n",
    "    prompt=f\"\"\"下面是一个模型，根据创作要求，完成的创作内容。按照规范性评分标准给模型创作打分(只取1分、2分、3分、4分、5分其一)。规范性评分标准如下： \n",
    "            1分: 创作内容离题，与提示语句要求不符，格式非常不规范，杂乱无章，句子结构混乱，缺乏逻辑，每千字平均错误数超过5个。 \n",
    "            2分: 创作内容与提示语句要求有一定契合但覆盖不全，格式较不规范：缺乏清晰的结构，但基本逻辑仍能找到，每千字平均错误数在4至5个之间。 \n",
    "            3分：创作内容与提示语句要求基本契合但覆盖不全，格式一般规范，结构基本顺畅，逻辑较清晰，每千字平均错误数在2至4个之间。 \n",
    "            4分：创作内容与提示语句要求基本契合且基本覆盖，格式较规范，结构清晰，逻辑条理分明，每千字平均错误数在1至2个之间。 \n",
    "            5分：创作内容与提示语句要求完美契合，格式非常规范：结构严谨，逻辑清晰，层次分明，每千字平均错误数在0至1个之间。\n",
    "            -->{text} \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"您将担任评审的专家角色\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 2048, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        \n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d974d21-da05-445e-bfdb-189c050bca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3049/3049 [08:23<00:00,  6.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "label = []\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    test_item = test_df.loc[i]\n",
    "    if (test_item['评判维度']==\"选择题\"):\n",
    "        test_input = f\"{test_item['创作要求']}。{test_item['待评判内容']}\"\n",
    "        label.append(int(predict1(test_input)))\n",
    "    elif (test_item['评判维度']==\"流畅性\"):\n",
    "        test_input = f\"模型创作:{test_item['待评判内容']}\"\n",
    "        label.append(int(predict2(test_input)))\n",
    "    else:\n",
    "        test_input = f\"创作要求:{test_item['创作要求']}。模型创作:{test_item['待评判内容']}\"\n",
    "        label.append(int(predict3(test_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e13b1bd-97e4-4462-842b-e458c5302212",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['预测分数'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b642e1fc-83be-4b72-9562-6cb18d24f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = test_df[['数据编号', '评判维度', '预测分数']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e3db413-3935-4200-ab40-558ae3ce76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('glm4_9b_1000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbaeae-0b49-4cd4-9143-a91d0f9df760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "label = []\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    test_item = test_df.loc[i]\n",
    "    if (test_item['评判维度']==\"选择题\"):\n",
    "        test_input = f\"{test_item['创作要求']}。{test_item['待评判内容']}\"\n",
    "        label.append(int(predict1(test_input)))\n",
    "    elif (test_item['评判维度']==\"流畅性\"):\n",
    "        test_input = f\"模型创作:{test_item['待评判内容']}\"\n",
    "        label.append(int(predict2(test_input)))\n",
    "    else:\n",
    "        test_input = f\"创作要求:{test_item['创作要求']}。模型创作:{test_item['待评判内容']}\"\n",
    "        label.append(int(predict3(test_input)))y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76ecd57-c82d-4cb4-952d-9b8be3ddd7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    " \n",
    " \n",
    "def apply_lora(model_name_or_path, output_path, lora_path):\n",
    "    print(f\"Loading the base model from {model_name_or_path}\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    "    )\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    " \n",
    "    print(f\"Loading the LoRA adapter from {lora_path}\")\n",
    " \n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base,\n",
    "        lora_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    " \n",
    "    print(\"Applying the LoRA\")\n",
    "    model = lora_model.merge_and_unload()\n",
    " \n",
    "    print(f\"Saving the target model to {output_path}\")\n",
    "    model.save_pretrained(output_path)\n",
    "    base_tokenizer.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf68dab8-ff32-4591-81d2-328f4a6dea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"/root/share/new_models/qwen/Qwen2-7B-Instruct\"\n",
    "output_path = \"/group_share/Qwen2_7B_instruct_review\"\n",
    "lora_path = \"/group_share/Qwen2_7B_instruct_lora/checkpoint-500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092c4f5c-be69-4fac-94e3-2621d84c1b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the base model from /root/share/new_models/qwen/Qwen2-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the LoRA adapter from /group_share/Qwen2_7B_instruct_lora/checkpoint-500\n",
      "Applying the LoRA\n",
      "Saving the target model to /group_share/Qwen2_7B_instruct_review\n"
     ]
    }
   ],
   "source": [
    "apply_lora(model_name_or_path, output_path, lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78953eef-068d-4658-bbbd-d2b77febeda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting modelscope\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/58/bd/963488515859b641a2aa525ed20282e914dd4d3dc9471002f11b94e4a016/modelscope-1.18.0-py3-none-any.whl (5.7 MB)\n",
      "Collecting urllib3>=1.26\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/ca/1c/89ffc63a9605b583d5df2be791a27bc1a42b7c32bab68d3c8f2f73a98cd4/urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Collecting requests>=2.25\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.8/dist-packages (from modelscope) (4.66.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.25->modelscope) (2.8)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/3d/09/d82fe4a34c5f0585f9ea1df090e2a71eb9bb1e469723053e1ee9f57c16f3/charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.25->modelscope) (2019.11.28)\n",
      "Installing collected packages: urllib3, charset-normalizer, requests, modelscope\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.8\n",
      "    Not uninstalling urllib3 at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'urllib3'. No files were found to uninstall.\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.22.0\n",
      "    Not uninstalling requests at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "Successfully installed charset-normalizer-3.3.2 modelscope-1.18.0 requests-2.32.3 urllib3-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee93c57b-c90e-425d-993b-cfd68c17f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 9 not upgraded.\n",
      "Need to get 3316 kB of archives.\n",
      "After this operation, 11.1 MB of additional disk space will be used.\n",
      "Get:1 https://mirrors.aliyun.com/ubuntu focal/universe amd64 git-lfs amd64 2.9.2-1 [3316 kB]\n",
      "Fetched 3316 kB in 4s (766 kB/s)   \u001b[0m33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package git-lfs.\n",
      "(Reading database ... 27010 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_2.9.2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking git-lfs (2.9.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up git-lfs (2.9.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cbc8d-34b5-4c5f-8b50-f136b31aef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.hub.api import HubApi\n",
    "\n",
    "YOUR_ACCESS_TOKEN = 'e65f0ae8-fd82-4626-a011-c1001e776ccd'\n",
    "\n",
    "api = HubApi()\n",
    "api.login(YOUR_ACCESS_TOKEN)\n",
    "api.push_model(\n",
    "    model_id=\"NumberJys/Qwen2_7B_instruct_review_500\", \n",
    "    model_dir=\"/group_share/Qwen2_7B_instruct_review\" # 本地模型目录，要求目录中必须包含configuration.json\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cc621-5440-440c-a49a-451d2f8ab2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
