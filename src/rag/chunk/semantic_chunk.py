# coding:utf-8
"""è¯­ä¹‰åˆ‡ç‰‡ç­–ç•¥åŸºäºå¥å­è¾¹ç•Œè¿›è¡Œåˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§"""
import re

def semantic_chunking(text, max_chunk_size=512):
    """åŸºäºè¯­ä¹‰çš„åˆ‡ç‰‡ - æŒ‰å¥å­åˆ†å‰²"""
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ\n]+', text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        # å¦‚æœå½“å‰å¥å­åŠ å…¥åè¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œä¿å­˜å½“å‰å—
        if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            current_chunk += " " + sentence if current_chunk else sentence
    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    return chunks

def print_chunk_analysis(chunks, method_name):
    """æ‰“å°åˆ‡ç‰‡åˆ†æç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ {method_name}")
    print(f"{'='*60}")
    if not chunks:
        print("âŒ æœªç”Ÿæˆä»»ä½•åˆ‡ç‰‡")
        return
    total_length = sum(len(chunk) for chunk in chunks)
    avg_length = total_length / len(chunks)
    min_length = min(len(chunk) for chunk in chunks)
    max_length = max(len(chunk) for chunk in chunks)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - åˆ‡ç‰‡æ•°é‡: {len(chunks)}")
    print(f"   - å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
    print(f"   - æœ€çŸ­é•¿åº¦: {min_length} å­—ç¬¦")
    print(f"   - æœ€é•¿é•¿åº¦: {max_length} å­—ç¬¦")
    print(f"   - é•¿åº¦æ–¹å·®: {max_length - min_length} å­—ç¬¦")
    print(f"\nğŸ“ åˆ‡ç‰‡å†…å®¹:")
    for i, chunk in enumerate(chunks, 1):
        print(f"   å— {i} ({len(chunk)} å­—ç¬¦):")
        print(f"   {chunk}")
        print()

# æµ‹è¯•æ–‡æœ¬
text = """è¿ªå£«å°¼ä¹å›­æä¾›å¤šç§é—¨ç¥¨ç±»å‹ä»¥æ»¡è¶³ä¸åŒæ¸¸å®¢éœ€æ±‚ã€‚ä¸€æ—¥ç¥¨æ˜¯æœ€åŸºç¡€çš„é—¨ç¥¨ç±»å‹ï¼Œå¯åœ¨è´­ä¹°æ—¶é€‰å®šæ—¥æœŸä½¿ç”¨ï¼Œä»·æ ¼æ ¹æ®å­£èŠ‚æµ®åŠ¨ã€‚ä¸¤æ—¥ç¥¨éœ€è¦è¿ç»­ä¸¤å¤©ä½¿ç”¨ï¼Œæ€»ä»·æ¯”è´­ä¹°ä¸¤å¤©å•æ—¥ç¥¨ä¼˜æƒ çº¦9æŠ˜ã€‚ç‰¹å®šæ—¥ç¥¨åŒ…å«éƒ¨åˆ†èŠ‚åº†æ´»åŠ¨æ—¶æ®µï¼Œéœ€æ³¨æ„é—¨ç¥¨æ ‡æ³¨çš„æœ‰æ•ˆæœŸé™ã€‚è´­ç¥¨æ¸ é“ä»¥å®˜æ–¹æ¸ é“ä¸ºä¸»ï¼ŒåŒ…æ‹¬ä¸Šæµ·è¿ªå£«å°¼å®˜ç½‘ã€å®˜æ–¹Appã€å¾®ä¿¡å…¬ä¼—å·åŠå°ç¨‹åºã€‚ç¬¬ä¸‰æ–¹å¹³å°å¦‚é£çŒªã€æºç¨‹ç­‰åˆä½œä»£ç†å•†ä¹Ÿå¯è´­ç¥¨ï¼Œä½†éœ€è®¤å‡†å®˜æ–¹æˆæƒæ ‡è¯†ã€‚æ‰€æœ‰ç”µå­ç¥¨éœ€ç»‘å®šèº«ä»½è¯ä»¶ï¼Œæ¸¯æ¾³å°å±…æ°‘å¯ç”¨é€šè¡Œè¯ï¼Œå¤–ç±æ¸¸å®¢ç”¨æŠ¤ç…§ï¼Œå„¿ç«¥ç¥¨éœ€æä¾›å‡ºç”Ÿè¯æ˜æˆ–æˆ·å£æœ¬å¤å°ä»¶ã€‚ç”Ÿæ—¥ç¦åˆ©éœ€åœ¨å®˜æ–¹æ¸ é“ç™»è®°ï¼Œå¯è·èµ ç”Ÿæ—¥å¾½ç« å’Œç”œå“åˆ¸ã€‚åŠå¹´å†…æœ‰æ•ˆç»“å©šè¯æŒæœ‰è€…å¯è´­ä¹°ç‰¹åˆ«å¥—ç¥¨ï¼Œå«çš‡å®¶å®´ä¼šå…åŒäººé¤ã€‚å†›äººä¼˜æƒ ç°å½¹åŠé€€å½¹å†›äººå‡­è¯ä»¶äº«8æŠ˜ï¼Œéœ€è‡³å°‘æå‰3å¤©ç™»è®°å®¡æ‰¹ã€‚"""

if __name__ == "__main__":
    print("ğŸ¯ è¯­ä¹‰åˆ‡ç‰‡ç­–ç•¥æµ‹è¯•")
    print(f"ğŸ“„ æµ‹è¯•æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    # ä½¿ç”¨è¯­ä¹‰åˆ‡ç‰‡
    chunks = semantic_chunking(text, max_chunk_size=300)
    print_chunk_analysis(chunks, "è¯­ä¹‰åˆ‡ç‰‡")